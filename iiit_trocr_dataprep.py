# -*- coding: utf-8 -*-
"""IIIT-TrOCR-DataPrep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UZUJESZTqMGCJNKsQoURcWLrHVK1GR15
"""

# IIT =- TrOCR
from transformers import VisionEncoderDecoderModel
from transformers import ViTFeatureExtractor, RobertaTokenizer, TrOCRProcessor
from transformers import VisionEncoderDecoderModel
from transformers import TrOCRProcessor
from PIL import Image


import matplotlib.pyplot as plt
from matplotlib import font_manager as fm


import os
import random
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

root_directory = '/content/drive/MyDrive/IIT-HW-Hindi_v1/'

os.listdir(root_directory)

#! tar -xvzf /content/drive/MyDrive/IIT-HW-Hindi_v1/HindiSeg.tar.gz -C /content/drive/MyDrive/IIT-HW-Hindi_v1

import os
import pandas as pd

# Specify the parent directory path
root_dir = 'HindiSeg/train' # Replace with your directory path

# Initialize an empty list to store file paths
file_paths = []

# Recursively traverse through all directories and subdirectories
for dirpath, dirnames, filenames in os.walk(root_dir):
    for filename in filenames:
        # Construct the full file path
        file_path = os.path.join(dirpath, filename)
        file_paths.append(file_path)

# Create a DataFrame with the file paths
df = pd.DataFrame(file_paths, columns=['File Path'])

# Display the DataFrame
print(df)

# Print the total number of files
print(f"\nTotal number of files: {len(df)}")

df['File Path'][1]



# Specify the parent directory path
root_dir =  'HindiSeg/test' # Replace with your directory path

# Initialize an empty list to store file paths
file_paths = []

# Recursively traverse through all directories and subdirectories
for dirpath, dirnames, filenames in os.walk(root_dir):
    for filename in filenames:
        # Construct the full file path
        file_path = os.path.join(dirpath, filename)
        file_paths.append(file_path)

# Create a DataFrame with the file paths
df2 = pd.DataFrame(file_paths, columns=['File Path'])

# Display the DataFrame
print(df2)

# Print the total number of files
print(f"\nTotal number of files: {len(df2)}")

# Specify the parent directory path
root_dir ='HindiSeg/val' # Replace with your directory path

# Initialize an empty list to store file paths
file_paths = []

# Recursively traverse through all directories and subdirectories
for dirpath, dirnames, filenames in os.walk(root_dir):
    for filename in filenames:
        # Construct the full file path
        file_path = os.path.join(dirpath, filename)
        file_paths.append(file_path)

# Create a DataFrame with the file paths
df3 = pd.DataFrame(file_paths, columns=['File Path'])

# Display the DataFrame
print(df3)

# Print the total number of files
print(f"\nTotal number of files: {len(df3)}")

len(df)+len(df2)+len(df3)

os.chdir(root_dir)

df=df[~df['File Path'].str.endswith('.txt','.py')].reset_index(drop=True)
df2=df2[~df['File Path'].str.endswith('.txt','.py')].reset_index(drop=True)

df3=df3[~df['File Path'].str.endswith('.txt','.py')].reset_index(drop=True)

df.to_csv('train_extracted_files.csv')
df2.to_csv('test_extracted_files.csv')
df3.to_csv('val_extracted_files.csv')

# since the extracted file and the txt file has not all the common files. we perform difference operations

# lets load the files of val,test and train txt to df

with open(root_directory +'val.txt') as f:
    val = f.readlines()
counter = 0
val_list = []
for i in range(len(val)):
    # if counter > 2000:
    #     break
    image_id = val[i].split("\n")[0].split(' ')[0].strip()
#     vocab_id = int(train[i].split(",")[1].strip())
    text = val[i].split("\n")[0].split(' ')[1].strip()
    row = [image_id, text]
    val_list.append(row)
    counter += 1

val_df = pd.DataFrame(val_list, columns=['file_name', 'text'])
val_df.head()

with open(root_directory +'test.txt') as f:
    test = f.readlines()

counter = 0
test_list = []
for i in range(len(test)):
    # if counter > 2000:
    #     break
    image_id = test[i].split("\n")[0].split(' ')[0].strip()
#     vocab_id = int(train[i].split(",")[1].strip())
    text = test[i].split("\n")[0].split(' ')[1].strip()
    row = [image_id, text]
    test_list.append(row)
    counter += 1

test_df = pd.DataFrame(test_list, columns=['file_name', 'text'])
test_df.head()

with open(root_directory+'train.txt') as f:
    train = f.readlines()

counter = 0

train_list = []
for i in range(len(train)):
    # if counter > 5000:
    #     break
    image_id = train[i].split("\n")[0].split(' ')[0].strip()
#     vocab_id = int(train[i].split(",")[1].strip())
    text = train[i].split("\n")[0].split(' ')[1].strip()
    row = [image_id, text]
    train_list.append(row)
    counter += 1

train_df = pd.DataFrame(train_list, columns=['file_name', 'text'])
train_df.head()

# the file paths included iin txt but not present on

def remove_unextracted(path,extract):
  path_set = set(path['file_name'])
  extract_set = set(extract['File Path'])
  return  extract_set - path_set

# def change_full_path(x,root_dir):
#   x['modifed_file_path'] = x['file_name'].apply(lambda x: root_dir + x )

def filter_existing_file_paths(train_df, df):
    """
    Filters 'train_df' to include only rows where the file paths are present in 'df'.

    Parameters:
    train_df (pd.DataFrame): DataFrame containing file paths and corresponding values.
    df (pd.DataFrame): DataFrame containing all existing file paths.

    Returns:
    pd.DataFrame: A new DataFrame with rows from 'train_df' where the file paths are present in 'df'.
    """
    # Extract the file paths from both DataFrames
    train_file_paths = set(train_df['file_name'])
    existing_file_paths = set(df['File Path'])

    # Find the intersection of file paths
    valid_file_paths = train_file_paths.intersection(existing_file_paths)

    # Filter 'train_df' to keep only rows with valid file paths
    filtered_train_df = train_df[train_df['file_name'].isin(valid_file_paths)].reset_index(drop=True)

    return filtered_train_df

valid_train_df = filter_existing_file_paths(train_df,df)
valid_test_df = filter_existing_file_paths(test_df,df2)
valid_val_df = filter_existing_file_paths(val_df,df3)

valid_train_df.to_csv('train_extracted_files.csv')
valid_test_df.to_csv('test_extracted_files.csv')
valid_val_df.to_csv('val_extracted_files.csv')

# thus the finally obtained DF with valid existing files are in terms of
valid_train_df.head(3)

valid_test_df.head(3)

valid_val_df.head(3)

# import torch
# from torch.utils.data import Dataset
# from PIL import Image

# class IAMDataset(Dataset):
#     def __init__(self, root_dir, df, processor, max_target_length=128):
#         self.root_dir = root_dir
#         self.df = df
#         self.processor = processor
#         self.max_target_length = max_target_length

#     def __len__(self):
#         return len(self.df)

#     def __getitem__(self, idx):
#         # get file name + text
#         file_name = self.df['file_name'][idx]
#         text = self.df['text'][idx]
#         # prepare image (i.e. resize + normalize)
#         image = Image.open(self.root_dir + file_name).convert("RGB")
#         pixel_values = self.processor(image, return_tensors="pt").pixel_values
#         # add labels (input_ids) by encoding the text
#         labels = self.processor.tokenizer(text,
#                                           padding="max_length",
#                                           max_length=self.max_target_length).input_ids
#         # important: make sure that PAD tokens are ignored by the loss function
#         labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]

#         encoding = {"pixel_values": pixel_values.squeeze(), "labels": torch.tensor(labels)}
# #         print(encoding)
#         return encoding

# from transformers import ViTFeatureExtractor, RobertaTokenizer, TrOCRProcessor, PreTrainedTokenizerFast
# from transformers import VisionEncoderDecoderModel
# from transformers import BertTokenizer, BertModel
# from transformers import AutoTokenizer, AutoModelForMaskedLM

# encode = 'google/vit-base-patch16-224-in21k'
# decode = 'amitness/nepbert'

# feature_extractor=ViTFeatureExtractor.from_pretrained(encode)
# tokenizer = RobertaTokenizer.from_pretrained(decode)
# # tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
# # tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')
# processor = TrOCRProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)

# from transformers import TrOCRProcessor

# train_dataset = IAMDataset(root_dir=root_directory ,
#                            df=valid_train_df,
#                            processor=processor)
# eval_dataset = IAMDataset(root_dir=root_directory,
#                            df=valid_test_df,
#                            processor=processor)

# model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encode, decode)

# # set special tokens used for creating the decoder_input_ids from the labels
# model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
# model.config.pad_token_id = processor.tokenizer.pad_token_id
# print(processor.tokenizer.pad_token_id)
# # make sure vocab size is set correctly
# model.config.vocab_size = model.config.decoder.vocab_size

# # config_decoder.is_decoder = True
# # config_decoder.add_cross_attention = True

# # set beam search parameters
# model.config.eos_token_id = processor.tokenizer.sep_token_id
# model.config.max_length = 64
# model.config.early_stopping = True
# model.config.no_repeat_ngram_size = 3
# model.config.length_penalty = 2.0
# model.config.num_beams = 4

# image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert("RGB")
# image

# # define CER metric /function
# !pip install Levenshtein
# # import Levenshtein

# def cer_metric(references, predictions):
#     """
#     Calculate the Character Error Rate (CER) between reference and hypothesis.

#     Args:
#         reference (str): The ground truth text.
#         predictions (str): The predicted text.

#     Returns:
#         float: The CER between reference and hypothesis.
#     """
#     # Compute the Levenshtein distance (edit distance)
#     distance = Levenshtein.distance(references, predictions)

#     # Calculate CER by dividing the distance by the number of characters in the reference
#     cer_value = distance / len(references) if len(references) > 0 else float('inf')

#     return cer_value

# def compute_metrics(pred):
#     labels_ids = pred.label_ids
#     pred_ids = pred.predictions

#     pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
#     labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id
#     label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)

#     cer = cer_metric(predictions=pred_str, references=label_str)

#     return {"cer": cer}

# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

# training_args = Seq2SeqTrainingArguments(
#     predict_with_generate=True,
#     evaluation_strategy="steps",
#     per_device_train_batch_size=8,
#     per_device_eval_batch_size=8,
#     output_dir=root_directory,
#     save_total_limit=3,
#     logging_steps=2,
#     save_steps=200,
#     eval_steps=100,
# )

# from transformers import default_data_collator

# # instantiate trainer
# trainer = Seq2SeqTrainer(
#     model=model,
#     tokenizer=processor.feature_extractor,
#     args=training_args,
#     compute_metrics=compute_metrics,
#     train_dataset=train_dataset,
#     eval_dataset=eval_dataset,
#     data_collator=default_data_collator,
# )
# trainer.train()

